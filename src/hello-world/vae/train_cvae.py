import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from cvae import CVAE
import json

# ==========================
#      –ü–ê–†–ê–ú–ï–¢–†–´
# ==========================
DATASET = "dataset"
SAVE = "cvae.pth"
META = "metadata.json"

IMG = 32
Z_DIM = 64
EPOCHS = 1000
LR = 1e-3
BATCH = 32

device = "cuda" if torch.cuda.is_available() else "cpu"

# ==========================
#  –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ / –∑–∞–≥—Ä—É–∑–∫–∞
# ==========================
transform = transforms.Compose([
    transforms.Lambda(lambda img: img.convert("RGB")),
    transforms.Resize((IMG, IMG)),
    transforms.ToTensor(),
])

ds = datasets.ImageFolder(DATASET, transform=transform)
loader = DataLoader(ds, batch_size=BATCH, shuffle=True)

CLASS_NAMES = ds.classes
NUM_CLASSES = len(CLASS_NAMES)

print("–ö–ª–∞—Å—Å–æ–≤:", NUM_CLASSES, CLASS_NAMES)

# ==========================
#  –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–•
# ==========================
json.dump({
    "classes": CLASS_NAMES,
    "num_classes": NUM_CLASSES,
    "z_dim": Z_DIM,
    "img": IMG
}, open(META, "w"))

def one_hot(i):
    v = torch.zeros(NUM_CLASSES)
    v[i] = 1
    return v

# ==========================
#     –°–û–ó–î–ê–Å–ú –ú–û–î–ï–õ–¨
# ==========================
model = CVAE(z_dim=Z_DIM, cond_dim=NUM_CLASSES, img_size=IMG).to(device)

# ==========================
#     –î–û–û–±—É—á–µ–Ω–∏–µ
# ==========================
if os.path.exists(SAVE):
    print("üîÑ –ù–∞–π–¥–µ–Ω —Å—Ç–∞—Ä—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
    model.load_state_dict(torch.load(SAVE, map_location=device))
    print("‚úî –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ!")
else:
    print("‚ö† –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")

opt = optim.Adam(model.parameters(), lr=LR)

# ==========================
#     –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
# ==========================
def loss_fn(r, x, mu, logvar):
    mse = nn.functional.mse_loss(r, x)
    kld = -0.5 * torch.mean(1 + logvar - mu**2 - logvar.exp())
    return mse + kld * 0.01


# ==========================
#     –û–ë–£–ß–ï–ù–ò–ï
# ==========================
for epoch in range(1, EPOCHS+1):
    total = 0

    for imgs, lbls in loader:
        imgs = imgs.to(device)
        cond = torch.stack([one_hot(i) for i in lbls]).to(device)

        rec, mu, logvar = model(imgs, cond)
        loss = loss_fn(rec, imgs, mu, logvar)

        opt.zero_grad()
        loss.backward()
        opt.step()

        total += loss.item()

    print(f"Epoch {epoch}: {total / len(loader):.6f}")

    if epoch % 5 == 0:
        torch.save(model.state_dict(), SAVE)
        print("üíæ Checkpoint saved")

torch.save(model.state_dict(), SAVE)
print("üéâ TRAINING FINISHED")
